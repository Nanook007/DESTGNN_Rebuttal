---
# start up
start_up:
  # =================== running mode (select one of the three) ================== #
  mode: scratch     # three mode: test, resume, scratch
  resume_epoch: 0   # default to zero, if use the 'resume' mode, users need to set the epoch to resume.

  model_name:   D2STGNN                                   # model name
  load_pkl:     False                                     # load serialized dataloader

# Data Processing
data_args:
  dataset_name:   PEMS07
  num_nodes:      883
  data_dir:       datasets/PEMS07                         # data path
  adj_data_path:  datasets/sensor_graph/adj_PEMS07.pkl     # adj data path
  adj_type:       doubletransition                        # adj type to preprocessing

# Model Args
model_args:
  batch_size:   64
  num_feat:     1
  num_hidden:   32
  node_hidden:  12
  time_emb_dim: 12
  seq_length:   12

  node_dim: 40
  topk: 20

  conv_channels: 32 
  residual_channels: 32
  skip_channels: 32
  end_channels: 128

# TCN
  num_nodes: 883
  input_len: 12
  input_dim: 3
  in_dim: 3
  output_len: 12
  num_layer: 3

  # embed_dim: 32
  # node_dim_tcn: 32
  # temp_dim_tid: 32
  # temp_dim_diw: 32

  embed_dim: 128
  node_dim_tcn: 128
  temp_dim_tid: 128
  temp_dim_diw: 128

  time_of_day_size: 288
  day_of_week_size: 7
  if_T_i_D: True
  if_D_i_W: True
  if_node: True

# graph_constructor
  gcn_depth_ge: 2
  list_weight: [0.05, 0.95, 0.95]
  dims_1: 16
  dims_2: 32

  gcn_depth: 2
  gcn_layers: 1
  dropout: 0.2

# Optimization Args
optim_args:
  # adam optimizer
  lrate:          0.002                                 # learning rate
  print_model:    False
  wdecay:         1.0e-7                                  # weight decay of adam
  eps:            1.0e-8                                  # eps of adam
  
  # learning rate scheduler
  lr_decay_ways:  step
  # lr_decay_ways:  cos
  # lr_decay_ways:  ExLR
  exp_gamma: 0.95

  lr_schedule:    True                                    
  T_max: 10
  # lr_sche_steps:  [25, 40, 60, 90, 110, 130, 150, 170, 190]
  lr_sche_steps:  [50, 90, 120, 150, 170, 190]
  # lr_sche_steps:  [30, 60, 90, 120, 150, 170, 190]
  lr_decay_ratio: 0.5

# lrate: 0.002 wdecay: 1e-07 dropout: 0.2 lr_sche_steps: [50, 90, 120, 150, 170, 190] best
  
  # curriculum learning
  # if_cl:          True
  if_cl:          False
  cl_epochs:      6
  output_seq_len: 12
  
  # warm up
  warm_epochs:    0

  epochs:         100
  # epochs:         200
  # epochs:         1
  tolerance:      30
  patience:       30                                     
  seq_length:     12                                      